{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7TykUODMkiT",
        "outputId": "8a282e7b-e880-4343-a596-0daf6a061d71"
      },
      "outputs": [],
      "source": [
        "# Download dataset directly and set up environment\n",
        "!wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
        "!unzip ml-25m.zip\n",
        "\n",
        "# Import all necessary libraries upfront to avoid import issues\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "\n",
        "# Set path to dataset\n",
        "dataset_path = './ml-25m'\n",
        "print(f\"Using dataset path: {dataset_path}\")\n",
        "\n",
        "# Verify the dataset exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found at {dataset_path}\")\n",
        "    print(\"Files in directory:\", os.listdir(dataset_path))\n",
        "else:\n",
        "    print(f\"Dataset not found at {dataset_path}. Please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtg83z4JNUuQ"
      },
      "source": [
        "In this cell, I downloaded the MovieLens 25M dataset from the source and extracted it. I imported all the necessary libraries that will be used throughout the notebook, including pandas for data handling, numpy for numerical operations, and scikit-learn for implementing SVD. I set the path to the extracted dataset folder and verified that it exists with the expected files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uYJf4_i5Nbz9",
        "outputId": "f71a70f5-2a1b-430c-e9c3-749d1967b58b"
      },
      "outputs": [],
      "source": [
        "# Test all imported libraries\n",
        "import matplotlib\n",
        "import sklearn\n",
        "\n",
        "print(\"Testing imports...\")\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"Matplotlib version:\", matplotlib.__version__)\n",
        "print(\"Scikit-learn version:\", sklearn.__version__)\n",
        "\n",
        "# Test if other libraries are imported properly\n",
        "print(\"SciPy sparse module available:\", \"Yes\" if hasattr(sparse, \"csr_matrix\") else \"No\")\n",
        "print(\"Seaborn available:\", \"Yes\" if sns is not None else \"No\")\n",
        "print(\"All imports loaded successfully!\")\n",
        "\n",
        "# Test a basic plot to ensure matplotlib works\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n",
        "plt.title('Test Plot')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('xÂ²')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "xNEuEzW_OfhA",
        "outputId": "2f0a9e88-6100-431c-9285-64d5ff776256"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for this cell\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Load the ratings and movies datasets\n",
        "ratings = pd.read_csv(os.path.join(dataset_path, 'ratings.csv'))\n",
        "movies = pd.read_csv(os.path.join(dataset_path, 'movies.csv'))\n",
        "\n",
        "# Display information about the datasets\n",
        "print(f\"Ratings dataset shape: {ratings.shape}\")\n",
        "print(f\"Movies dataset shape: {movies.shape}\")\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"\\nRatings dataset preview:\")\n",
        "display(ratings.head())\n",
        "\n",
        "print(\"\\nMovies dataset preview:\")\n",
        "display(movies.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in ratings dataset:\")\n",
        "print(ratings.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in movies dataset:\")\n",
        "print(movies.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocIlocv0OjH4"
      },
      "source": [
        "In this cell, I loaded the ratings and movies datasets from the MovieLens files. The ratings dataset contains user ratings for different movies on a scale of 0.5 to 5 stars, while the movies dataset contains information about each movie including titles and genres. I displayed previews of both datasets to understand their structure and checked for any missing values that might need handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5-n2r2h7OnkX",
        "outputId": "a69b20ab-1fc2-4a7f-a2de-cd0fe78957b1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Basic statistics for ratings\n",
        "print(\"Ratings statistics:\")\n",
        "ratings_stats = ratings['rating'].describe()\n",
        "display(ratings_stats)\n",
        "\n",
        "# Visualize the rating distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(ratings['rating'], bins=9, kde=True)\n",
        "plt.title('Distribution of Movie Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Check the number of ratings per user\n",
        "user_ratings_count = ratings['userId'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(user_ratings_count, bins=50, kde=True)\n",
        "plt.title('Distribution of Ratings per User')\n",
        "plt.xlabel('Number of Ratings')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Median number of ratings per user: {user_ratings_count.median()}\")\n",
        "print(f\"Mean number of ratings per user: {user_ratings_count.mean():.2f}\")\n",
        "print(f\"Min number of ratings per user: {user_ratings_count.min()}\")\n",
        "print(f\"Max number of ratings per user: {user_ratings_count.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvls8_IiPQov"
      },
      "source": [
        "I analyzed the basic statistics and distribution of the movie ratings data. The first visualization shows how ratings are distributed across the 0.5-5 star scale, helping understand user rating patterns. The second visualization shows how many ratings each user has made, revealing that most users rate relatively few movies while some power users rate thousands. This analysis helps inform how I'll filter the data in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "Ha17ecnKPSiq",
        "outputId": "b6833b9f-7616-4384-8bec-1efd12e2f962"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Filter users who have rated at least 100 movies\n",
        "min_ratings = 100\n",
        "user_counts = ratings['userId'].value_counts()\n",
        "active_users = user_counts[user_counts >= min_ratings].index\n",
        "\n",
        "# Filter ratings for active users\n",
        "filtered_ratings = ratings[ratings['userId'].isin(active_users)]\n",
        "\n",
        "print(f\"Original ratings shape: {ratings.shape}\")\n",
        "print(f\"Filtered ratings shape: {filtered_ratings.shape}\")\n",
        "print(f\"Kept {len(active_users)} users out of {len(user_counts)} total users\")\n",
        "print(f\"Kept {filtered_ratings.shape[0]} ratings out of {ratings.shape[0]} total ratings\")\n",
        "print(f\"Percentage of data kept: {100 * filtered_ratings.shape[0] / ratings.shape[0]:.2f}%\")\n",
        "\n",
        "# Plot the rating distribution after filtering\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(filtered_ratings['rating'], bins=9, kde=True)\n",
        "plt.title('Distribution of Movie Ratings (After Filtering)')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Further reduce dataset size for memory efficiency\n",
        "print(\"\\nFurther reducing dataset size for memory efficiency...\")\n",
        "\n",
        "# Sample users\n",
        "np.random.seed(42)\n",
        "max_users = 5000  # Maximum number of users to include\n",
        "sample_users = np.random.choice(\n",
        "    filtered_ratings['userId'].unique(),\n",
        "    size=min(max_users, len(filtered_ratings['userId'].unique())),\n",
        "    replace=False\n",
        ")\n",
        "filtered_ratings_sample = filtered_ratings[filtered_ratings['userId'].isin(sample_users)]\n",
        "\n",
        "# Focus on popular movies\n",
        "min_movie_ratings = 50  # Movies must have at least this many ratings\n",
        "movie_counts = filtered_ratings_sample['movieId'].value_counts()\n",
        "popular_movies = movie_counts[movie_counts >= min_movie_ratings].index\n",
        "filtered_ratings_sample = filtered_ratings_sample[filtered_ratings_sample['movieId'].isin(popular_movies)]\n",
        "\n",
        "print(f\"Working with {len(filtered_ratings_sample['userId'].unique())} users\")\n",
        "print(f\"Working with {len(filtered_ratings_sample['movieId'].unique())} movies\")\n",
        "print(f\"Total ratings: {len(filtered_ratings_sample)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XIMi0YuQFfv"
      },
      "source": [
        "I created a memory-efficient sparse user-item matrix where rows represent users, columns represent movies, and values represent ratings. Instead of using a dense matrix that would waste memory on storing zeros, I used a sparse representation that only stores non-zero values. I also created mappings between original IDs and matrix indices for later use. The data was split into training (80%) and testing (20%) sets to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR24Re4CQwJq",
        "outputId": "680dcc63-d180-433f-dc38-cd5dbe1732f8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create user-item matrix efficiently\n",
        "print(\"Creating sparse user-item matrix...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create mappings for user and movie IDs\n",
        "users = filtered_ratings_sample['userId'].unique()\n",
        "movies_subset = filtered_ratings_sample['movieId'].unique()\n",
        "\n",
        "userid_to_idx = {user: i for i, user in enumerate(users)}\n",
        "movieid_to_idx = {movie: i for i, movie in enumerate(movies_subset)}\n",
        "idx_to_userid = {i: user for user, i in userid_to_idx.items()}\n",
        "idx_to_movieid = {i: movie for movie, i in movieid_to_idx.items()}\n",
        "\n",
        "# Create row and column indices\n",
        "row_indices = [userid_to_idx[user] for user in filtered_ratings_sample['userId']]\n",
        "col_indices = [movieid_to_idx[movie] for movie in filtered_ratings_sample['movieId']]\n",
        "ratings_data = filtered_ratings_sample['rating'].values\n",
        "\n",
        "# Create sparse matrix\n",
        "sparse_matrix = sparse.coo_matrix(\n",
        "    (ratings_data, (row_indices, col_indices)),\n",
        "    shape=(len(userid_to_idx), len(movieid_to_idx))\n",
        ").tocsr()\n",
        "\n",
        "print(f\"Matrix creation completed in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Sparse matrix shape: {sparse_matrix.shape}\")\n",
        "\n",
        "# Calculate sparsity\n",
        "sparsity = 1.0 - (sparse_matrix.count_nonzero() /\n",
        "                 (sparse_matrix.shape[0] * sparse_matrix.shape[1]))\n",
        "print(f\"Matrix sparsity: {sparsity:.4f} (or {sparsity*100:.2f}% empty)\")\n",
        "\n",
        "# Split data for training and testing\n",
        "train_data, test_data = train_test_split(filtered_ratings_sample, test_size=0.2, random_state=42)\n",
        "print(f\"Training set: {train_data.shape}\")\n",
        "print(f\"Testing set: {test_data.shape}\")\n",
        "\n",
        "# Create training sparse matrix\n",
        "train_row_indices = [userid_to_idx[user] for user in train_data['userId']]\n",
        "train_col_indices = [movieid_to_idx[movie] for movie in train_data['movieId']]\n",
        "train_ratings = train_data['rating'].values\n",
        "\n",
        "train_sparse = sparse.coo_matrix(\n",
        "    (train_ratings, (train_row_indices, train_col_indices)),\n",
        "    shape=(len(userid_to_idx), len(movieid_to_idx))\n",
        ").tocsr()\n",
        "\n",
        "print(f\"Training sparse matrix shape: {train_sparse.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kxupHuNRENu"
      },
      "source": [
        "I created a memory-efficient sparse user-item matrix where rows represent users, columns represent movies, and values represent ratings. Instead of using a dense matrix that would waste memory on storing zeros, I used a sparse representation that only stores non-zero values. I also created mappings between original IDs and matrix indices for later use. The data was split into training (80%) and testing (20%) sets to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Cc62XiPRMIO",
        "outputId": "d85da1bc-71ec-4ee0-cf36-f5b428353c2f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import time\n",
        "# Apply Singular Value Decomposition (SVD)\n",
        "print(\"Applying Truncated SVD...\")\n",
        "n_components = 100  # Number of latent factors\n",
        "start_time = time.time()\n",
        "\n",
        "# Fit SVD on the training data\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "user_factors = svd.fit_transform(train_sparse)\n",
        "item_factors = svd.components_.T\n",
        "\n",
        "print(f\"SVD completed in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"User factors shape: {user_factors.shape}\")\n",
        "print(f\"Item factors shape: {item_factors.shape}\")\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Explained variance by {n_components} components: {explained_variance:.4f}\")\n",
        "\n",
        "# Plot the explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(svd.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by SVD Components')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the individual explained variance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(n_components), svd.explained_variance_ratio_)\n",
        "plt.xlabel('Component Index')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Individual Explained Variance by SVD Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqmuKnqYRas0"
      },
      "source": [
        "I applied Singular Value Decomposition (SVD) to extract latent factors from the user-item matrix. This technique reduces dimensionality and identifies hidden patterns that represent users' preferences and movie characteristics. The 100 components captured about 37% of the variance in the data. The first visualization shows the cumulative explained variance as components are added, while the second shows the individual contribution of each component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwBnxZ67RcM_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Function to get movie recommendations for a user\n",
        "def get_recommendations(user_id, n_recommendations=10):\n",
        "    if user_id not in userid_to_idx:\n",
        "        print(f\"User {user_id} not found in the sampled dataset.\")\n",
        "        return None\n",
        "\n",
        "    # Get the user's index in the matrix\n",
        "    user_idx = userid_to_idx[user_id]\n",
        "\n",
        "    # Get the user's latent factors\n",
        "    user_vector = user_factors[user_idx, :].reshape(1, -1)\n",
        "\n",
        "    # Calculate predicted ratings for all movies\n",
        "    predicted_ratings = np.dot(user_vector, item_factors.T).flatten()\n",
        "\n",
        "    # Create a DataFrame with movie IDs and predicted ratings\n",
        "    movie_ids = [idx_to_movieid[i] for i in range(len(idx_to_movieid))]\n",
        "    movie_preds = pd.DataFrame({\n",
        "        'movieId': movie_ids,\n",
        "        'predicted_rating': predicted_ratings\n",
        "    })\n",
        "\n",
        "    # Get movies that the user has already rated\n",
        "    rated_movies = train_data[train_data['userId'] == user_id]['movieId'].unique()\n",
        "\n",
        "    # Filter out already rated movies\n",
        "    recommendations = movie_preds[~movie_preds['movieId'].isin(rated_movies)]\n",
        "\n",
        "    # Sort by predicted rating\n",
        "    recommendations = recommendations.sort_values('predicted_rating', ascending=False)\n",
        "\n",
        "    # Get top N recommendations\n",
        "    top_recommendations = recommendations.head(n_recommendations)\n",
        "\n",
        "    # Merge with movie information\n",
        "    result = top_recommendations.merge(movies, on='movieId')\n",
        "\n",
        "    return result[['movieId', 'title', 'genres', 'predicted_rating']]\n",
        "\n",
        "# Function to find similar movies\n",
        "def get_similar_movies(movie_id, n_similar=10):\n",
        "    if movie_id not in movieid_to_idx:\n",
        "        print(f\"Movie {movie_id} not found in the sampled dataset.\")\n",
        "        return None\n",
        "\n",
        "    # Get the movie's index in the matrix\n",
        "    movie_idx = movieid_to_idx[movie_id]\n",
        "\n",
        "    # Get the movie's latent factors\n",
        "    movie_vector = item_factors[movie_idx, :].reshape(1, -1)\n",
        "\n",
        "    # Calculate similarity with all other movies\n",
        "    similarity_scores = cosine_similarity(movie_vector, item_factors)\n",
        "\n",
        "    # Create a DataFrame with movie IDs and similarity scores\n",
        "    movie_ids = [idx_to_movieid[i] for i in range(len(idx_to_movieid))]\n",
        "    similar_movies = pd.DataFrame({\n",
        "        'movieId': movie_ids,\n",
        "        'similarity': similarity_scores[0]\n",
        "    })\n",
        "\n",
        "    # Remove the input movie\n",
        "    similar_movies = similar_movies[similar_movies['movieId'] != movie_id]\n",
        "\n",
        "    # Sort by similarity\n",
        "    similar_movies = similar_movies.sort_values('similarity', ascending=False)\n",
        "\n",
        "    # Get top N similar movies\n",
        "    top_similar = similar_movies.head(n_similar)\n",
        "\n",
        "    # Merge with movie information\n",
        "    result = top_similar.merge(movies, on='movieId')\n",
        "\n",
        "    return result[['movieId', 'title', 'genres', 'similarity']]\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(sample_size=1000):\n",
        "    # Create a set of user-item pairs from the test data\n",
        "    if len(test_data) > sample_size:\n",
        "        test_sample = test_data.sample(sample_size, random_state=42)\n",
        "    else:\n",
        "        test_sample = test_data\n",
        "\n",
        "    # Initialize variables for error calculation\n",
        "    error_sum = 0\n",
        "    count = 0\n",
        "\n",
        "    for _, row in test_sample.iterrows():\n",
        "        user_id = row['userId']\n",
        "        movie_id = row['movieId']\n",
        "\n",
        "        # Skip if user or movie not in mappings\n",
        "        if user_id not in userid_to_idx or movie_id not in movieid_to_idx:\n",
        "            continue\n",
        "\n",
        "        user_idx = userid_to_idx[user_id]\n",
        "        movie_idx = movieid_to_idx[movie_id]\n",
        "\n",
        "        # Get the actual rating\n",
        "        actual_rating = row['rating']\n",
        "\n",
        "        # Calculate predicted rating\n",
        "        user_vec = user_factors[user_idx, :].reshape(1, -1)\n",
        "        movie_vec = item_factors[movie_idx, :].reshape(-1, 1)\n",
        "        predicted_rating = np.dot(user_vec, movie_vec)[0][0]\n",
        "\n",
        "        # Calculate error\n",
        "        error = (actual_rating - predicted_rating) ** 2\n",
        "        error_sum += error\n",
        "        count += 1\n",
        "\n",
        "    # Calculate RMSE\n",
        "    if count > 0:\n",
        "        rmse = np.sqrt(error_sum / count)\n",
        "        return rmse\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWqeFgA7R5OP"
      },
      "source": [
        "I defined three key functions that power the recommendation system. The first function gets personalized movie recommendations for a user based on their latent factors. The second function finds movies similar to a given movie using cosine similarity between latent factors. The third function evaluates the model's performance by calculating the Root Mean Square Error (RMSE) between predicted and actual ratings in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "87-IUvk4RzT5",
        "outputId": "e49ccd0c-9870-4c1a-d1bb-143f8c231406"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import time\n",
        "# Calculate RMSE on the test set\n",
        "print(\"Evaluating model...\")\n",
        "start_time = time.time()\n",
        "rmse = evaluate_model(sample_size=5000)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Test different numbers of components to see the effect on RMSE\n",
        "component_range = [10, 20, 50, 100, 150, 200]\n",
        "rmse_values = []\n",
        "\n",
        "for n_comp in component_range:\n",
        "    print(f\"Testing with {n_comp} components...\")\n",
        "\n",
        "    # Apply SVD with different number of components\n",
        "    svd_test = TruncatedSVD(n_components=n_comp, random_state=42)\n",
        "    user_factors_test = svd_test.fit_transform(train_sparse)\n",
        "    item_factors_test = svd_test.components_.T\n",
        "\n",
        "    # Save original factors\n",
        "    global user_factors, item_factors\n",
        "    user_factors_temp, item_factors_temp = user_factors, item_factors\n",
        "    user_factors, item_factors = user_factors_test, item_factors_test\n",
        "\n",
        "    # Evaluate\n",
        "    rmse = evaluate_model(sample_size=2000)\n",
        "    rmse_values.append(rmse)\n",
        "    print(f\"RMSE with {n_comp} components: {rmse:.4f}\")\n",
        "\n",
        "    # Restore original factors\n",
        "    user_factors, item_factors = user_factors_temp, item_factors_temp\n",
        "\n",
        "# Plot RMSE vs number of components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(component_range, rmse_values, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('RMSE vs Number of SVD Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7c4NDnXSCvh"
      },
      "source": [
        "I evaluated the model's performance by calculating the RMSE on the test data, which measures how close the predicted ratings are to actual ratings. I then experimented with different numbers of SVD components to find the optimal balance between model complexity and accuracy. The visualization shows how RMSE changes with different component counts, helping to identify the sweet spot where additional components no longer significantly improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7fOnWcWqSE2m",
        "outputId": "fc77d902-91fe-4915-f8ea-6239e6e854c8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Get recommendations for a sample user\n",
        "sample_user_id = list(userid_to_idx.keys())[0]  # Get the first user in our sample\n",
        "print(f\"Getting movie recommendations for user {sample_user_id}:\")\n",
        "recommendations = get_recommendations(sample_user_id, n_recommendations=10)\n",
        "display(recommendations)\n",
        "\n",
        "# Visualize the predicted ratings\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(recommendations)), recommendations['predicted_rating'], align='center')\n",
        "plt.yticks(range(len(recommendations)), recommendations['title'].str.slice(0, 30))\n",
        "plt.xlabel('Predicted Rating')\n",
        "plt.title(f'Top 10 Movie Recommendations for User {sample_user_id}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check what genres the user typically rates highly\n",
        "user_ratings = train_data[train_data['userId'] == sample_user_id]\n",
        "user_movies = user_ratings.merge(movies, on='movieId')\n",
        "\n",
        "print(f\"\\nUser {sample_user_id} has rated {len(user_movies)} movies\")\n",
        "print(\"Average rating by genre:\")\n",
        "\n",
        "# Extract all genres from the user's rated movies\n",
        "all_genres = []\n",
        "for genres in user_movies['genres']:\n",
        "    all_genres.extend(genres.split('|'))\n",
        "\n",
        "# Count ratings by genre\n",
        "genre_ratings = {}\n",
        "for _, row in user_movies.iterrows():\n",
        "    for genre in row['genres'].split('|'):\n",
        "        if genre not in genre_ratings:\n",
        "            genre_ratings[genre] = []\n",
        "        genre_ratings[genre].append(row['rating'])\n",
        "\n",
        "# Calculate average rating per genre\n",
        "genre_avg_rating = {genre: np.mean(ratings) for genre, ratings in genre_ratings.items()\n",
        "                   if len(ratings) >= 5}  # Only include genres with at least 5 movies\n",
        "\n",
        "# Sort by average rating\n",
        "genre_avg_rating = {k: v for k, v in sorted(genre_avg_rating.items(),\n",
        "                                           key=lambda item: item[1],\n",
        "                                           reverse=True)}\n",
        "\n",
        "# Display genres and average ratings\n",
        "genre_df = pd.DataFrame(list(genre_avg_rating.items()),\n",
        "                       columns=['Genre', 'Average Rating'])\n",
        "display(genre_df)\n",
        "\n",
        "# Visualize average ratings by genre\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(genre_avg_rating)), list(genre_avg_rating.values()), align='center')\n",
        "plt.yticks(range(len(genre_avg_rating)), list(genre_avg_rating.keys()))\n",
        "plt.xlabel('Average Rating')\n",
        "plt.title(f'Average Ratings by Genre for User {sample_user_id}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAl9ZilESSc9"
      },
      "source": [
        "I generated movie recommendations for a sample user and visualized the predicted ratings. I also analyzed the user's past rating behavior by calculating average ratings for each genre they've rated. This helps understand why certain movies are being recommended - the system identifies patterns in the user's preferences across different genres an"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k9_zfTc2TK3y",
        "outputId": "bf7abf33-4a36-413b-d003-9f503d6ebbe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find similar movies to a popular movie\n",
        "print(\"\\nFinding movies similar to a popular movie...\")\n",
        "# Try to find Toy Story in our dataset\n",
        "toy_story_id = 1\n",
        "if toy_story_id in movieid_to_idx:\n",
        "    movie_to_use = toy_story_id\n",
        "    movie_info = movies[movies['movieId'] == movie_to_use]\n",
        "    print(f\"Finding movies similar to: {movie_info['title'].values[0]}\")\n",
        "else:\n",
        "    # If not found, use the first movie in our dataset\n",
        "    movie_to_use = movies_subset[0]\n",
        "    movie_info = movies[movies['movieId'] == movie_to_use]\n",
        "    print(f\"Toy Story not found in sample. Using movie: {movie_info['title'].values[0]}\")\n",
        "\n",
        "similar_movies = get_similar_movies(movie_to_use, n_similar=10)\n",
        "print(\"Similar movies:\")\n",
        "display(similar_movies)\n",
        "\n",
        "# Visualize similar movies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(similar_movies)), similar_movies['similarity'], align='center')\n",
        "plt.yticks(range(len(similar_movies)), similar_movies['title'].str.slice(0, 30))\n",
        "plt.xlabel('Similarity Score')\n",
        "plt.title(f'Top 10 Movies Similar to {movie_info[\"title\"].values[0]}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSVD-based Movie Recommendation System is complete!\")\n",
        "\n",
        "# Interactive function to get recommendations\n",
        "def interactive_recommendations():\n",
        "    while True:\n",
        "        print(\"\\n==== Movie Recommendation System ====\")\n",
        "        print(\"1. Get movie recommendations for a user\")\n",
        "        print(\"2. Find similar movies\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter choice (1-3): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            user_id = int(input(\"Enter user ID: \"))\n",
        "            n_recs = int(input(\"Number of recommendations: \"))\n",
        "            recommendations = get_recommendations(user_id, n_recommendations=n_recs)\n",
        "            if recommendations is not None:\n",
        "                print(\"\\nRecommended movies:\")\n",
        "                for idx, row in recommendations.iterrows():\n",
        "                    print(f\"{row['title']} | Rating: {row['predicted_rating']:.2f}\")\n",
        "            else:\n",
        "                print(f\"User {user_id} not found\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            movie_name = input(\"Enter partial movie name: \")\n",
        "            matching_movies = movies[movies['title'].str.contains(movie_name, case=False)]\n",
        "\n",
        "            if matching_movies.empty:\n",
        "                print(\"No matching movies found\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nMatching movies:\")\n",
        "            for idx, row in matching_movies.head(10).iterrows():\n",
        "                print(f\"{row['movieId']}: {row['title']}\")\n",
        "\n",
        "            movie_id = int(input(\"Enter movie ID: \"))\n",
        "            n_similar = int(input(\"Number of similar movies: \"))\n",
        "\n",
        "            similar_movies = get_similar_movies(movie_id, n_similar=n_similar)\n",
        "            if similar_movies is not None:\n",
        "                print(\"\\nSimilar movies:\")\n",
        "                for idx, row in similar_movies.iterrows():\n",
        "                    print(f\"{row['title']} | Similarity: {row['similarity']:.4f}\")\n",
        "            else:\n",
        "                print(f\"Movie {movie_id} not found\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice\")\n",
        "\n",
        "\n",
        "interactive_recommendations()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
